{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0sbVvQs1MnBi"
   },
   "outputs": [],
   "source": [
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "\n",
    "consumer_key = 't7JkWdi85BVHiyRETXxKhAZVw'\n",
    "consumer_secret = 'dd4CV3evkURrThiqDOGD3QCaY2BhUXboqONfDHcTtzrSGJ9ebb'\n",
    "access_token = '1272816278319792128-vJcUJMq188VV61bEYMU1wP2hn9DlN8'\n",
    "access_secret = 'cWFbORJfT7eh8lYxQ2tuIG6DWVMELjUtJA9Mfi87iF3ZM'\n",
    "\n",
    "f = open('collectedTweets.txt', 'a')\n",
    "    \n",
    "#This is a basic listener that just prints received tweets to stdout.\n",
    "class StdOutListener(StreamListener):\n",
    "\n",
    "    def __init__(self):\n",
    "      StreamListener.__init__(self)\n",
    "      self.max_tweets = 5000\n",
    "      self.tweet_count = 0\n",
    "\n",
    "    def on_data(self, data):\n",
    "        if (self.tweet_count == self.max_tweets):\n",
    "            print(\"Terminated\")\n",
    "            return False\n",
    "        else:\n",
    "            self.tweet_count += 1\n",
    "            #print(data)\n",
    "            f.write(data)\n",
    "            return True\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    #This handles Twitter authetification and the connection to Twitter Streaming API\n",
    "    l = StdOutListener()\n",
    "    auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_secret)\n",
    "    stream = Stream(auth, l)\n",
    "\n",
    "    #This line filter Twitter Streams to capture data by the keywords: \n",
    "    stream.filter(track=['science'], languages=[\"en\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c8E_fynS5nIy",
    "outputId": "5e0101f6-969b-4111-d680-412f9cca1010"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n",
    "def remove_stopwordsAndTokenize(text): \n",
    "    all_stopwords = stopwords.words('english')\n",
    "    #sw_to_add = ['\"\"',\"''\",\":\",'``','’','-',\",\",'|',\".\",'”', '“',\"?\",\"&\",\"!\",\"'\"]\n",
    "    #all_stopwords.extend(sw_to_add)\n",
    "    text_tokens = word_tokenize(text)\n",
    "    tokens_without_sw = [word for word in text_tokens if not word in all_stopwords]\n",
    "    return tokens_without_sw\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"\\S*https?:\\S*\", \"\", text) #delete web links\n",
    "    text = remove_emoji(text)\n",
    "    text = re.sub(\"\\n\",\" \",text) #delete back to the line\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"rt\\s*@[^:]*: \",\"\",text) #delete rt and contact\n",
    "    #text = re.sub(\"@[a-zA-Z0-9-]\\S* \",\"\",text) #remove all contact\n",
    "    text = re.sub(\"@[a-zA-Z0-9-]\\S*\",\"\",text) #remove all contact\n",
    "    #text = re.sub(\"#[a-zA-Z0-9-]\\S*\",\"\",text) #remove all hashtags\n",
    "    #text = re.sub(\"#\",\"\",text) #remove all hashtags\n",
    "    text = re.sub(\"r&amp;d\",\"research and development\",text)\n",
    "    text = re.sub(\"&amp;\",\"&\",text)\n",
    "    text = re.sub('[^a-zA-Z0-9 \\n\\.]', ' ', text)\n",
    "    text = re.sub('[.]', ' ', text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def lemmatize(token_list):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    token_list = [wordnet_lemmatizer.lemmatize(word, pos=\"v\") for word in token_list]\n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_LDA(text):\n",
    "    text = clean_text(text)\n",
    "    tokens = remove_stopwordsAndTokenize(text)\n",
    "    tokens = lemmatize(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def loadTweets():\n",
    "    with open(\"collectedTweets.txt\", 'r', encoding=\"utf-8\") as jsonfile:\n",
    "        data = [json.loads(l) for l in jsonfile.readlines() if len(l) > 5]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def storeTextTweet():\n",
    "    data = loadTweets()\n",
    "\n",
    "    tweetTextFile = open('tweetsText.txt', 'a')\n",
    "    text_data = []\n",
    "    for tweet in data:\n",
    "        if(tweet[\"truncated\"]):\n",
    "            text = tweet[\"extended_tweet\"][\"full_text\"]\n",
    "        else:\n",
    "            text = tweet[\"text\"]\n",
    "        tweetTextFile.write(text)\n",
    "        tweetTextFile.write(\"\\n\")\n",
    "    \n",
    "    tweetTextFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getText_data():\n",
    "    data = loadTweets()\n",
    "    text_data = []\n",
    "    for tweet in data:\n",
    "        if(tweet[\"truncated\"]):\n",
    "            text = tweet[\"extended_tweet\"][\"full_text\"]\n",
    "        else:\n",
    "            text = tweet[\"text\"]\n",
    "        token = prepare_data_for_LDA(text)\n",
    "        text_data.append(token)\n",
    "    return text_data\n",
    "    #print(text_data)\n",
    "text_data = getText_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fMjY1A3H5zdT",
    "outputId": "0d972b70-8ef9-4185-c33e-3ec1565ea720"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "NUM_TOPICS = 5\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
    "ldamodel.save('model5.gensim')\n",
    "topics = ldamodel.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 582
    },
    "id": "4tQm4XOy8r8n",
    "outputId": "afe7af50-5b85-414a-ca77-4de06a955c48",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "# feed the LDA model into the pyLDAvis instance\n",
    "lda_viz = gensimvis.prepare(ldamodel, corpus, dictionary)\n",
    "lda_viz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FD92i1qQGsEF"
   },
   "source": [
    "Compute Model Perplexity and Coherence Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OiSdAO_VGrtZ",
    "outputId": "f58c461b-d478-4989-f201-4417f97da06c"
   },
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=ldamodel, texts=text_data, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score for 5 (=K) topics: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Projet B tweetstest.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
